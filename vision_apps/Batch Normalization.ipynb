{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the tutorial for batch normalization on the same sample MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999,)\n"
     ]
    }
   ],
   "source": [
    "#Data Locations\n",
    "train_path = './data/mnist_example/digitstrain.csv'\n",
    "val_path = './data/mnist_example/digitsvalid.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "X_train = np.asarray(train_data.values[:,:784])\n",
    "Y_train = np.asarray(train_data.values[:,784])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs], name = \"X\")\n",
    "training = tf.placeholder_with_default(True, shape = (), name = \"training\")\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method-1\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = \"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = \"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn= tf.layers.dense(bn2_act, n_outputs, name = \"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training= training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method-2 : to avoid repitition\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = \"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = \"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = \"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,\n",
    "                                                             logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate= learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 , Validation Accuracy : 0.7117117\n",
      "Epoch : 1 , Validation Accuracy : 0.7827828\n",
      "Epoch : 2 , Validation Accuracy : 0.8158158\n",
      "Epoch : 3 , Validation Accuracy : 0.8298298\n",
      "Epoch : 4 , Validation Accuracy : 0.8338338\n",
      "Epoch : 5 , Validation Accuracy : 0.8398398\n",
      "Epoch : 6 , Validation Accuracy : 0.8488488\n",
      "Epoch : 7 , Validation Accuracy : 0.8598599\n",
      "Epoch : 8 , Validation Accuracy : 0.8638639\n",
      "Epoch : 9 , Validation Accuracy : 0.8688689\n",
      "Epoch : 10 , Validation Accuracy : 0.8708709\n",
      "Epoch : 11 , Validation Accuracy : 0.8738739\n",
      "Epoch : 12 , Validation Accuracy : 0.8778779\n",
      "Epoch : 13 , Validation Accuracy : 0.8798799\n",
      "Epoch : 14 , Validation Accuracy : 0.8838839\n",
      "Epoch : 15 , Validation Accuracy : 0.8838839\n",
      "Epoch : 16 , Validation Accuracy : 0.8868869\n",
      "Epoch : 17 , Validation Accuracy : 0.8898899\n",
      "Epoch : 18 , Validation Accuracy : 0.8948949\n",
      "Epoch : 19 , Validation Accuracy : 0.8958959\n",
      "Epoch : 20 , Validation Accuracy : 0.8948949\n",
      "Epoch : 21 , Validation Accuracy : 0.8918919\n",
      "Epoch : 22 , Validation Accuracy : 0.8958959\n",
      "Epoch : 23 , Validation Accuracy : 0.8958959\n",
      "Epoch : 24 , Validation Accuracy : 0.8968969\n",
      "Epoch : 25 , Validation Accuracy : 0.9009009\n",
      "Epoch : 26 , Validation Accuracy : 0.9009009\n",
      "Epoch : 27 , Validation Accuracy : 0.9009009\n",
      "Epoch : 28 , Validation Accuracy : 0.8988989\n",
      "Epoch : 29 , Validation Accuracy : 0.8978979\n",
      "Epoch : 30 , Validation Accuracy : 0.9029029\n",
      "Epoch : 31 , Validation Accuracy : 0.9019019\n",
      "Epoch : 32 , Validation Accuracy : 0.9049049\n",
      "Epoch : 33 , Validation Accuracy : 0.9029029\n",
      "Epoch : 34 , Validation Accuracy : 0.9039039\n",
      "Epoch : 35 , Validation Accuracy : 0.9049049\n",
      "Epoch : 36 , Validation Accuracy : 0.9049049\n",
      "Epoch : 37 , Validation Accuracy : 0.9049049\n",
      "Epoch : 38 , Validation Accuracy : 0.9089089\n",
      "Epoch : 39 , Validation Accuracy : 0.9059059\n",
      "Epoch : 40 , Validation Accuracy : 0.9059059\n",
      "Epoch : 41 , Validation Accuracy : 0.9069069\n",
      "Epoch : 42 , Validation Accuracy : 0.9069069\n",
      "Epoch : 43 , Validation Accuracy : 0.9089089\n",
      "Epoch : 44 , Validation Accuracy : 0.9049049\n",
      "Epoch : 45 , Validation Accuracy : 0.9079079\n",
      "Epoch : 46 , Validation Accuracy : 0.9049049\n",
      "Epoch : 47 , Validation Accuracy : 0.9079079\n",
      "Epoch : 48 , Validation Accuracy : 0.9079079\n",
      "Epoch : 49 , Validation Accuracy : 0.9069069\n",
      "Epoch : 50 , Validation Accuracy : 0.9109109\n",
      "Epoch : 51 , Validation Accuracy : 0.9069069\n",
      "Epoch : 52 , Validation Accuracy : 0.9069069\n",
      "Epoch : 53 , Validation Accuracy : 0.9059059\n",
      "Epoch : 54 , Validation Accuracy : 0.9089089\n",
      "Epoch : 55 , Validation Accuracy : 0.9069069\n",
      "Epoch : 56 , Validation Accuracy : 0.9089089\n",
      "Epoch : 57 , Validation Accuracy : 0.9079079\n",
      "Epoch : 58 , Validation Accuracy : 0.9089089\n",
      "Epoch : 59 , Validation Accuracy : 0.9119119\n",
      "Epoch : 60 , Validation Accuracy : 0.9099099\n",
      "Epoch : 61 , Validation Accuracy : 0.9139139\n",
      "Epoch : 62 , Validation Accuracy : 0.9149149\n",
      "Epoch : 63 , Validation Accuracy : 0.9129129\n",
      "Epoch : 64 , Validation Accuracy : 0.9119119\n",
      "Epoch : 65 , Validation Accuracy : 0.9119119\n",
      "Epoch : 66 , Validation Accuracy : 0.9139139\n",
      "Epoch : 67 , Validation Accuracy : 0.9149149\n",
      "Epoch : 68 , Validation Accuracy : 0.9169169\n",
      "Epoch : 69 , Validation Accuracy : 0.9159159\n",
      "Epoch : 70 , Validation Accuracy : 0.9159159\n",
      "Epoch : 71 , Validation Accuracy : 0.9189189\n",
      "Epoch : 72 , Validation Accuracy : 0.9139139\n",
      "Epoch : 73 , Validation Accuracy : 0.9169169\n",
      "Epoch : 74 , Validation Accuracy : 0.9139139\n",
      "Epoch : 75 , Validation Accuracy : 0.9179179\n",
      "Epoch : 76 , Validation Accuracy : 0.9159159\n",
      "Epoch : 77 , Validation Accuracy : 0.9169169\n",
      "Epoch : 78 , Validation Accuracy : 0.9189189\n",
      "Epoch : 79 , Validation Accuracy : 0.9189189\n",
      "Epoch : 80 , Validation Accuracy : 0.9179179\n",
      "Epoch : 81 , Validation Accuracy : 0.9179179\n",
      "Epoch : 82 , Validation Accuracy : 0.9189189\n",
      "Epoch : 83 , Validation Accuracy : 0.9179179\n",
      "Epoch : 84 , Validation Accuracy : 0.9209209\n",
      "Epoch : 85 , Validation Accuracy : 0.9199199\n",
      "Epoch : 86 , Validation Accuracy : 0.9209209\n",
      "Epoch : 87 , Validation Accuracy : 0.9209209\n",
      "Epoch : 88 , Validation Accuracy : 0.9199199\n",
      "Epoch : 89 , Validation Accuracy : 0.9209209\n",
      "Epoch : 90 , Validation Accuracy : 0.9199199\n",
      "Epoch : 91 , Validation Accuracy : 0.9189189\n",
      "Epoch : 92 , Validation Accuracy : 0.9209209\n",
      "Epoch : 93 , Validation Accuracy : 0.9199199\n",
      "Epoch : 94 , Validation Accuracy : 0.9209209\n",
      "Epoch : 95 , Validation Accuracy : 0.9199199\n",
      "Epoch : 96 , Validation Accuracy : 0.9189189\n",
      "Epoch : 97 , Validation Accuracy : 0.9199199\n",
      "Epoch : 98 , Validation Accuracy : 0.9189189\n",
      "Epoch : 99 , Validation Accuracy : 0.9219219\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "val_data = pd.read_csv(val_path)\n",
    "test = val_data.values \n",
    "train_data = train_data.values\n",
    "np.random.shuffle(train_data)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        np.random.shuffle(train_data)\n",
    "        ind = 0\n",
    "        while ind < train_data.shape[0]:\n",
    "            X_batch = train_data[ind:ind+batch_size, :784]\n",
    "            y_batch = train_data[ind:ind+batch_size,784]\n",
    "            sess.run(training_op, feed_dict = {X : X_batch, y : y_batch})\n",
    "            ind += batch_size\n",
    "        acc_val = accuracy.eval(feed_dict = {X : test[:,:784], y : test[:,784]})\n",
    "        print(\"Epoch : {!r} , Validation Accuracy : {!r}\".format(epoch, acc_val))\n",
    "        #print ( \"\" % epoch, acc_val)\n",
    "    save_path = saver.save(sess, './trained_models/batch_norm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
