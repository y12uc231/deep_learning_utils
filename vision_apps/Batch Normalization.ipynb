{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the tutorial for batch normalization on the same sample MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999,)\n"
     ]
    }
   ],
   "source": [
    "#Data Locations\n",
    "train_path = './data/mnist_example/digitstrain.csv'\n",
    "val_path = './data/mnist_example/digitsvalid.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "X_train = np.asarray(train_data.values[:,:784])\n",
    "Y_train = np.asarray(train_data.values[:,784])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs], name = \"X\")\n",
    "training = tf.placeholder_with_default(True, shape = (), name = \"training\")\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method-1\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = \"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training = training, momentum = 0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = \"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training = training, momentum = 0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn= tf.layers.dense(bn2_act, n_outputs, name = \"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training= training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note :Run only one of the methods, else it will throw a scope error\n",
    "#Method-2 : to avoid repitition .\n",
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name = \"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name = \"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name = \"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y,\n",
    "                                                             logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate= learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 , Validation Accuracy : 0.6846847\n",
      "Epoch : 1 , Validation Accuracy : 0.7877878\n",
      "Epoch : 2 , Validation Accuracy : 0.8228228\n",
      "Epoch : 3 , Validation Accuracy : 0.8368368\n",
      "Epoch : 4 , Validation Accuracy : 0.8498498\n",
      "Epoch : 5 , Validation Accuracy : 0.8578579\n",
      "Epoch : 6 , Validation Accuracy : 0.8648649\n",
      "Epoch : 7 , Validation Accuracy : 0.8678679\n",
      "Epoch : 8 , Validation Accuracy : 0.8718719\n",
      "Epoch : 9 , Validation Accuracy : 0.8748749\n",
      "Epoch : 10 , Validation Accuracy : 0.8768769\n",
      "Epoch : 11 , Validation Accuracy : 0.8808809\n",
      "Epoch : 12 , Validation Accuracy : 0.8818819\n",
      "Epoch : 13 , Validation Accuracy : 0.8828829\n",
      "Epoch : 14 , Validation Accuracy : 0.8818819\n",
      "Epoch : 15 , Validation Accuracy : 0.8808809\n",
      "Epoch : 16 , Validation Accuracy : 0.8848849\n",
      "Epoch : 17 , Validation Accuracy : 0.8878879\n",
      "Epoch : 18 , Validation Accuracy : 0.8878879\n",
      "Epoch : 19 , Validation Accuracy : 0.8878879\n",
      "Epoch : 20 , Validation Accuracy : 0.8948949\n",
      "Epoch : 21 , Validation Accuracy : 0.8948949\n",
      "Epoch : 22 , Validation Accuracy : 0.8948949\n",
      "Epoch : 23 , Validation Accuracy : 0.8988989\n",
      "Epoch : 24 , Validation Accuracy : 0.8968969\n",
      "Epoch : 25 , Validation Accuracy : 0.8978979\n",
      "Epoch : 26 , Validation Accuracy : 0.9049049\n",
      "Epoch : 27 , Validation Accuracy : 0.9009009\n",
      "Epoch : 28 , Validation Accuracy : 0.9009009\n",
      "Epoch : 29 , Validation Accuracy : 0.9039039\n",
      "Epoch : 30 , Validation Accuracy : 0.9019019\n",
      "Epoch : 31 , Validation Accuracy : 0.9079079\n",
      "Epoch : 32 , Validation Accuracy : 0.9049049\n",
      "Epoch : 33 , Validation Accuracy : 0.9079079\n",
      "Epoch : 34 , Validation Accuracy : 0.9049049\n",
      "Epoch : 35 , Validation Accuracy : 0.9129129\n",
      "Epoch : 36 , Validation Accuracy : 0.9079079\n",
      "Epoch : 37 , Validation Accuracy : 0.9129129\n",
      "Epoch : 38 , Validation Accuracy : 0.9119119\n",
      "Epoch : 39 , Validation Accuracy : 0.9169169\n",
      "Epoch : 40 , Validation Accuracy : 0.9099099\n",
      "Epoch : 41 , Validation Accuracy : 0.9129129\n",
      "Epoch : 42 , Validation Accuracy : 0.9129129\n",
      "Epoch : 43 , Validation Accuracy : 0.9139139\n",
      "Epoch : 44 , Validation Accuracy : 0.9129129\n",
      "Epoch : 45 , Validation Accuracy : 0.9129129\n",
      "Epoch : 46 , Validation Accuracy : 0.9109109\n",
      "Epoch : 47 , Validation Accuracy : 0.9139139\n",
      "Epoch : 48 , Validation Accuracy : 0.9139139\n",
      "Epoch : 49 , Validation Accuracy : 0.9119119\n",
      "Epoch : 50 , Validation Accuracy : 0.9119119\n",
      "Epoch : 51 , Validation Accuracy : 0.9099099\n",
      "Epoch : 52 , Validation Accuracy : 0.9139139\n",
      "Epoch : 53 , Validation Accuracy : 0.9119119\n",
      "Epoch : 54 , Validation Accuracy : 0.9119119\n",
      "Epoch : 55 , Validation Accuracy : 0.9129129\n",
      "Epoch : 56 , Validation Accuracy : 0.9119119\n",
      "Epoch : 57 , Validation Accuracy : 0.9139139\n",
      "Epoch : 58 , Validation Accuracy : 0.9129129\n",
      "Epoch : 59 , Validation Accuracy : 0.9119119\n",
      "Epoch : 60 , Validation Accuracy : 0.9129129\n",
      "Epoch : 61 , Validation Accuracy : 0.9149149\n",
      "Epoch : 62 , Validation Accuracy : 0.9119119\n",
      "Epoch : 63 , Validation Accuracy : 0.9149149\n",
      "Epoch : 64 , Validation Accuracy : 0.9139139\n",
      "Epoch : 65 , Validation Accuracy : 0.9139139\n",
      "Epoch : 66 , Validation Accuracy : 0.9149149\n",
      "Epoch : 67 , Validation Accuracy : 0.9129129\n",
      "Epoch : 68 , Validation Accuracy : 0.9109109\n",
      "Epoch : 69 , Validation Accuracy : 0.9119119\n",
      "Epoch : 70 , Validation Accuracy : 0.9109109\n",
      "Epoch : 71 , Validation Accuracy : 0.9139139\n",
      "Epoch : 72 , Validation Accuracy : 0.9159159\n",
      "Epoch : 73 , Validation Accuracy : 0.9149149\n",
      "Epoch : 74 , Validation Accuracy : 0.9159159\n",
      "Epoch : 75 , Validation Accuracy : 0.9179179\n",
      "Epoch : 76 , Validation Accuracy : 0.9159159\n",
      "Epoch : 77 , Validation Accuracy : 0.9149149\n",
      "Epoch : 78 , Validation Accuracy : 0.9149149\n",
      "Epoch : 79 , Validation Accuracy : 0.9169169\n",
      "Epoch : 80 , Validation Accuracy : 0.9139139\n",
      "Epoch : 81 , Validation Accuracy : 0.9189189\n",
      "Epoch : 82 , Validation Accuracy : 0.9179179\n",
      "Epoch : 83 , Validation Accuracy : 0.9149149\n",
      "Epoch : 84 , Validation Accuracy : 0.9159159\n",
      "Epoch : 85 , Validation Accuracy : 0.9209209\n",
      "Epoch : 86 , Validation Accuracy : 0.9189189\n",
      "Epoch : 87 , Validation Accuracy : 0.9189189\n",
      "Epoch : 88 , Validation Accuracy : 0.9159159\n",
      "Epoch : 89 , Validation Accuracy : 0.9199199\n",
      "Epoch : 90 , Validation Accuracy : 0.9189189\n",
      "Epoch : 91 , Validation Accuracy : 0.9179179\n",
      "Epoch : 92 , Validation Accuracy : 0.9189189\n",
      "Epoch : 93 , Validation Accuracy : 0.9199199\n",
      "Epoch : 94 , Validation Accuracy : 0.9189189\n",
      "Epoch : 95 , Validation Accuracy : 0.9159159\n",
      "Epoch : 96 , Validation Accuracy : 0.9189189\n",
      "Epoch : 97 , Validation Accuracy : 0.9199199\n",
      "Epoch : 98 , Validation Accuracy : 0.9209209\n",
      "Epoch : 99 , Validation Accuracy : 0.9199199\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "val_data = pd.read_csv(val_path)\n",
    "test = val_data.values \n",
    "train_data = train_data.values\n",
    "np.random.shuffle(train_data)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    accs = []\n",
    "    for epoch in tqdm(range(100)):\n",
    "        np.random.shuffle(train_data)\n",
    "        ind = 0\n",
    "        while ind < train_data.shape[0]:\n",
    "            X_batch = train_data[ind:ind+batch_size, :784]\n",
    "            y_batch = train_data[ind:ind+batch_size,784]\n",
    "            sess.run(training_op, feed_dict = {X : X_batch, y : y_batch, training: True})\n",
    "            ind += batch_size\n",
    "        accs.append(accuracy.eval(feed_dict = {X : test[:,:784], y : test[:,784]}))\n",
    "    for i in range(len(accs)):\n",
    "        print(\"Epoch : {!r} , Validation Accuracy : {!r}\".format(i, accs[i]))\n",
    "    #print ( \"\" % epoch, acc_val)\n",
    "    save_path = saver.save(sess, './trained_models/batch_norm.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
